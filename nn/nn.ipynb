{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import fnmatch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_image_from_zip(zip_path, img_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        with zf.open(img_path) as file:\n",
    "            img = Image.open(file)\n",
    "            return img.convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "        \n",
    "class ZipImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, zip_path, root, transform=None):\n",
    "        self.zip_path = zip_path\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.classes = ['0_real', '1_fake']\n",
    "        self.img_paths = self._get_image_paths()\n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        img_paths = []\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zf:\n",
    "            for file_info in zf.infolist():\n",
    "                name = file_info.filename\n",
    "                if fnmatch.fnmatch(name, f\"{self.root}/*.jpg\"):\n",
    "                    label = 0 if '0_real' in name.split('/')[1] else 1\n",
    "                    img_paths.append((name, label))\n",
    "        return img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.img_paths[index]\n",
    "        img = load_image_from_zip(self.zip_path, img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(zip_path, batch_size, image_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_dir = \"AIGC-Detection-Dataset/train\"\n",
    "    val_dir = \"AIGC-Detection-Dataset/val\"\n",
    "    test_dir = \"AIGC-Detection-Dataset/val\"\n",
    "\n",
    "    train_dataset = ZipImageFolder(zip_path, train_dir, transform=transform)\n",
    "    val_dataset = ZipImageFolder(zip_path, val_dir, transform=transform)\n",
    "    test_dataset = ZipImageFolder(zip_path, test_dir, transform=transform)\n",
    "    print(f\"Data prepared:\\nTrain: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"Data loaded\")\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.resnet50()\n",
    "    model.fc = nn.Linear(model.fc.in_features, 1)  # Binary classification\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE).float().unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predicted = (outputs > 0.5).int()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        batch = 1\n",
    "        print(f\"Number of batches: {len(train_loader)}\")\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE).float().unsqueeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            print(f\"Batch {batch} complete, Loss: {loss.item():.4f}\")\n",
    "            batch += 1\n",
    "            \n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        # Validation\n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping: if validation loss hasn't improved, stop training\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            # Save the best model weights (optional)\n",
    "            save_model(model)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE).float().unsqueeze(1)\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Data prepared:\n",
      "Train: 45000, Val: 5000, Test: 5000\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    zip_path = \"../AIGC-Detection-Dataset.zip\"\n",
    "    batch_size = 32\n",
    "    image_size = 512\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    train_loader, val_loader, test_loader = prepare_data(zip_path, batch_size, image_size)\n",
    "\n",
    "    # Model building\n",
    "    model = build_model()\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 1\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "    # Testing\n",
    "    test_model(model, test_loader)\n",
    "\n",
    "    # Save the model\n",
    "    save_model(model, \"nn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
