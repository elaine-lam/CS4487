{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS4487\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "import zipfile\n",
    "import fnmatch\n",
    "from PIL import Image\n",
    "\n",
    "#For texture extraction\n",
    "from skimage import feature\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texture_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    gray_image = gray_image.astype(np.uint8)\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = feature.local_binary_pattern(gray_image, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=224, range=(0, 256))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist\n",
    "\n",
    "def extract_color_features(image):\n",
    "    hist_b = cv2.calcHist([image], [0], None, [224], [0, 256]).flatten()\n",
    "    hist_g = cv2.calcHist([image], [1], None, [224], [0, 256]).flatten()\n",
    "    hist_r = cv2.calcHist([image], [2], None, [224], [0, 256]).flatten()\n",
    "    hist = np.concatenate([hist_b, hist_g, hist_r])\n",
    "    hist /= (hist.sum() + 1e-6)  # Normalize\n",
    "    return hist[:224]\n",
    "\n",
    "def extract_shape_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    gray_image = gray_image.astype(np.uint8)  # Ensure the image is of type uint8\n",
    "    \n",
    "    edges = cv2.Canny(gray_image, 100, 200)\n",
    "    hist, _ = np.histogram(edges.ravel(), bins=224, range=(0, 256))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_zip(zip_path, img_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        with zf.open(img_path) as file:\n",
    "            img = Image.open(file)\n",
    "            return img.convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "        \n",
    "class ZipImageFolderDataset(datasets.ImageFolder):\n",
    "    def __init__(self, zip_path, root, transform=None):\n",
    "        self.zip_path = zip_path\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.classes = ['0_real', '1_fake']\n",
    "        self.img_paths = self._get_image_paths()\n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        img_paths = []\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zf:\n",
    "            for file_info in zf.infolist():\n",
    "                name = file_info.filename\n",
    "                if fnmatch.fnmatch(name, f\"{self.root}/*.jpg\"):\n",
    "                    label = 0 if '0_real' in name.split('/')[1] else 1\n",
    "                    img_paths.append((name, label))\n",
    "        return img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.img_paths[index]\n",
    "        img = load_image_from_zip(self.zip_path, img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Ensure the image is now a tensor\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            raise TypeError(f\"Expected image to be a tensor, but got {type(img)}.\")\n",
    "        \n",
    "        # Convert tensor to numpy array for feature extraction\n",
    "        img_np = img.numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Extract features\n",
    "        texture_features = extract_texture_features(img_np).flatten()\n",
    "        color_features = extract_color_features(cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)).flatten()\n",
    "        shape_features = extract_shape_features(img_np).flatten()\n",
    "        \n",
    "        features = np.stack([texture_features, color_features, shape_features], axis=0)\n",
    "        features = torch.tensor(features).float()\n",
    "        features = features.unsqueeze(-1).repeat(1, 1, 224)\n",
    "        \n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            max, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path='testing.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    for name, param in model.state_dict().items():\n",
    "        with open(r\"weights.txt\",'a') as file:\n",
    "            file.write(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")  \n",
    "    with open(r\"weights.txt\",'a') as file:\n",
    "        file.write('-'*100 + '\\n') \n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path='testing.pth'):\n",
    "    model = timm.create_model('seresnext101_32x4d', pretrained=True)\n",
    "    model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        print(f\"Epoch {epoch+1} started...\")\n",
    "        print(f\"length of train_loader: {len(train_loader)}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        batch = 1\n",
    "        for features, labels in train_loader:\n",
    "            # images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            features = features.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            print(f'Batch {batch} completed...')\n",
    "            batch += 1\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Time: {time.time()-start_time:.2f}s')\n",
    "        save_model(model, path=f'testing_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(zip_path, batch_size, image_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_dir = \"AIGC-Detection-Dataset/train\"\n",
    "    val_dir = \"AIGC-Detection-Dataset/val\"\n",
    "    # test_dir = \"AIGC-Detection-Dataset/val\"\n",
    "\n",
    "    train_dataset = ZipImageFolderDataset(zip_path, train_dir, transform=transform)\n",
    "    val_dataset = ZipImageFolderDataset(zip_path, val_dir, transform=transform)\n",
    "    # test_dataset = ZipImageFolderDataset(zip_path, test_dir, transform=transform)\n",
    "    print(f\"Data prepared:\\nTrain: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "    print(\"Data loaded\")\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared:\n",
      "Train: 45000, Val: 5000\n",
      "Data loaded\n",
      "Epoch 1 started...\n",
      "length of train_loader: 45000\n",
      "(224,) (224,) (224,)\n",
      "Batch 1 completed...\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('seresnext101_32x4d', pretrained=True)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Modify the first convolutional layer\n",
    "original_conv1 = model.conv1\n",
    "new_conv1 = nn.Conv2d(3, original_conv1.out_channels, kernel_size=original_conv1.kernel_size,\n",
    "                      stride=original_conv1.stride, padding=original_conv1.padding, bias=False)\n",
    "with torch.no_grad():\n",
    "    new_conv1.weight[:, :3, :, :] = original_conv1.weight[:, :3, :, :]\n",
    "model.conv1 = new_conv1\n",
    "\n",
    "# Load the data\n",
    "zip_path = '..\\AIGC-Detection-Dataset.zip'\n",
    "batch_size = 32\n",
    "image_size = 224\n",
    "train_loader, val_loader = load_data(zip_path, batch_size, image_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Adjust the final layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "train_model(model, train_loader, optimizer, criterion, 5)\n",
    "save_model(model, 'seresnext_finetuned.pth')\n",
    "evaluate(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
