{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS4487\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import metrics\n",
    "\n",
    "import zipfile\n",
    "import fnmatch\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "\n",
    "#For texture extraction\n",
    "from skimage import feature\n",
    "import os\n",
    "import csv\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texture_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    gray_image = gray_image.astype(np.uint8)  # Convert to integer type\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = feature.local_binary_pattern(gray_image, n_points, radius, method='uniform')\n",
    "\n",
    "    lbp = lbp / lbp.max()  # Normalize the LBP values to the range [0, 1]\n",
    "    \n",
    "    return lbp\n",
    "\n",
    "def extract_color_features(image, quality=95, enhance_factor=10):\n",
    "    temp_path = \"temp_recompressed.jpg\"\n",
    "    image.save(temp_path, format=\"JPEG\", quality=quality)\n",
    "    with Image.open(temp_path) as recompressed:\n",
    "        ela_image = ImageChops.difference(image, recompressed)\n",
    "    os.remove(temp_path)\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(ela_image)\n",
    "    enhanced_ela = enhancer.enhance(enhance_factor)\n",
    "\n",
    "    resized_ela = enhanced_ela.resize((224, 224)).convert(\"L\")\n",
    "    feature_array = np.array(resized_ela).astype(np.float32) / 255.0\n",
    "    return feature_array\n",
    "\n",
    "\n",
    "def extract_shape_features(image):\n",
    "    kernel_size = 5\n",
    "    transform_iteration = 5\n",
    "\n",
    "    # Define the kernel\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "    image = cv2.resize(image, (224, 224))  # Resize to (224, 224)\n",
    "\n",
    "    image_dict = {}\n",
    "    image_dict[\"original_image\"] = image\n",
    "    image_dict[\"eroded_image\"] = cv2.erode(image_dict[\"original_image\"], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"dilated_image\"] = cv2.dilate(image_dict[\"original_image\"], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"opened_image\"] = cv2.dilate(image_dict['eroded_image'], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"closed_image\"] = cv2.erode(image_dict['dilated_image'], kernel, iterations=transform_iteration)\n",
    "\n",
    "    opened_image_resized = cv2.cvtColor(image_dict[\"opened_image\"], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return opened_image_resized  # Shape: (224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_zip(zip_path, img_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        with zf.open(img_path) as file:\n",
    "            img = Image.open(file)\n",
    "            return img.convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "        \n",
    "class ZipImageFolderDataset(datasets.ImageFolder):\n",
    "    def __init__(self, zip_path, root, transform=None):\n",
    "        self.zip_path = zip_path\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.classes = ['0_real', '1_fake']\n",
    "        self.img_paths = self._get_image_paths()\n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        img_paths = []\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zf:\n",
    "            for file_info in zf.infolist():\n",
    "                name = file_info.filename\n",
    "                if fnmatch.fnmatch(name, f\"{self.root}/*.jpg\"):\n",
    "                    label = 0 if '0_real' in name.split('/')[1] else 1\n",
    "                    img_paths.append((name, label))\n",
    "        return img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.img_paths[index]\n",
    "        img = load_image_from_zip(self.zip_path, img_path)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img)\n",
    "        \n",
    "        # Ensure the image is now a tensor\n",
    "        if not isinstance(img_tensor, torch.Tensor):\n",
    "            raise TypeError(f\"Expected image to be a tensor, but got {type(img_tensor)}.\")\n",
    "        \n",
    "        # Convert tensor to numpy array for feature extraction\n",
    "        img_np = img_tensor.numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Extract features\n",
    "        texture_features = extract_texture_features(img_np)\n",
    "        color_features = extract_color_features(img)\n",
    "        shape_features = extract_shape_features(img_np)\n",
    "        \n",
    "        features = np.stack([texture_features, color_features, shape_features], axis=0)\n",
    "        features = torch.tensor(features).float().permute(1, 2, 0)  # Change the shape to [height, width, channels]\n",
    "        features = features.permute(2, 0, 1)  # Change the shape to [channels, height, width]\n",
    "        \n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    batch = 0\n",
    "    print(f\"Total batches: {len(test_loader)}\")\n",
    "    for img, label in test_loader:\n",
    "        # Please make sure that the \"pred\" is binary result\n",
    "        output = model(img.to(DEVICE))\n",
    "        pred = np.argmax(output.detach().to('cpu'), axis=1).numpy()\n",
    "        \n",
    "        y_true.extend(label.numpy())\n",
    "        y_pred.extend(pred)\n",
    "        \n",
    "        print(f\"Batch: {batch} completed\")\n",
    "        batch += 1\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    print(f'Validation Accuracy: {accuracy}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE).float().unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predicted = (outputs > 0.5).int()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path='testing.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    # for name, param in model.state_dict().items():\n",
    "    #     with open(r\"weights.txt\",'a') as file:\n",
    "    #         file.write(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")  \n",
    "    # with open(r\"weights.txt\",'a') as file:\n",
    "    #     file.write('-'*100 + '\\n') \n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path='testing.pth'):\n",
    "    model = timm.create_model('seresnext101_32x4d', pretrained=True)\n",
    "    model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        print(f\"Epoch {epoch+1} started...\")\n",
    "        print(f\"length of train_loader: {len(train_loader)}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        batch = 1\n",
    "        for features, labels in train_loader:\n",
    "            # images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            features = features.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            print(f'Batch {batch} completed...')\n",
    "            batch += 1\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Time: {time.time()-start_time:.2f}s')\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            # Save the best model weights (optional)\n",
    "            save_model(model)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss.\")\n",
    "            break\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(zip_path, batch_size, image_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_dir = \"AIGC-Detection-Dataset/train\"\n",
    "    val_dir = \"AIGC-Detection-Dataset/val\"\n",
    "    # test_dir = \"AIGC-Detection-Dataset/val\"\n",
    "\n",
    "    train_dataset = ZipImageFolderDataset(zip_path, train_dir, transform=transform)\n",
    "    val_dataset = ZipImageFolderDataset(zip_path, val_dir, transform=transform)\n",
    "    # test_dataset = ZipImageFolderDataset(zip_path, test_dir, transform=transform)\n",
    "    print(f\"Data prepared:\\nTrain: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "    print(\"Data loaded\")\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(train_losses, val_losses, val_accuracies, path='results.csv'):\n",
    "    with open(path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train Loss', 'Val Loss', 'Val Accuracy'])\n",
    "        for i in range(len(train_losses)):\n",
    "            writer.writerow([i+1, train_losses[i], val_losses[i], val_accuracies[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared:\n",
      "Train: 45000, Val: 5000\n",
      "Data loaded\n",
      "Epoch 1 started...\n",
      "length of train_loader: 45000\n",
      "Batch 1 completed...\n",
      "Batch 2 completed...\n",
      "Batch 3 completed...\n",
      "Batch 4 completed...\n",
      "Batch 5 completed...\n",
      "Batch 6 completed...\n",
      "Batch 7 completed...\n",
      "Batch 8 completed...\n",
      "Batch 9 completed...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = timm.create_model('seresnext101_32x4d', pretrained=True)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Modify the first convolutional layer\n",
    "original_conv1 = model.conv1\n",
    "new_conv1 = nn.Conv2d(3, original_conv1.out_channels, kernel_size=original_conv1.kernel_size,\n",
    "                      stride=original_conv1.stride, padding=original_conv1.padding, bias=False)\n",
    "with torch.no_grad():\n",
    "    new_conv1.weight[:, :3, :, :] = original_conv1.weight[:, :3, :, :]\n",
    "model.conv1 = new_conv1\n",
    "\n",
    "# Load the data\n",
    "zip_path = '..\\AIGC-Detection-Dataset.zip'\n",
    "batch_size = 64\n",
    "image_size = 224\n",
    "train_loader, val_loader = load_data(zip_path, batch_size, image_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Adjust the final layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "train_losses, val_losses, val_accuracies = train_model(model, train_loader, val_loader, optimizer, criterion, 5)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "save_results(train_losses, val_losses, val_accuracies)\n",
    "\n",
    "save_model(model, 'seresnext_finetuned.pth')\n",
    "# evaluate(model, train_loader)\n",
    "# evaluate(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
