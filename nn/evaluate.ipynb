{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS4487\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import fnmatch\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "\n",
    "#For texture extraction\n",
    "from skimage import feature\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texture_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    gray_image = gray_image.astype(np.uint8)  # Convert to integer type\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = feature.local_binary_pattern(gray_image, n_points, radius, method='uniform')\n",
    "\n",
    "    lbp = lbp / lbp.max()  # Normalize the LBP values to the range [0, 1]\n",
    "    \n",
    "    return lbp\n",
    "\n",
    "def extract_color_features(image, quality=95, enhance_factor=10):\n",
    "    image_bytes = image.tobytes()\n",
    "    recompressed = Image.frombytes(\"RGB\", image.size, image_bytes)\n",
    "    ela_image = ImageChops.difference(image, recompressed)\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(ela_image)\n",
    "    enhanced_ela = enhancer.enhance(enhance_factor)\n",
    "\n",
    "    resized_ela = enhanced_ela.resize((224, 224)).convert(\"L\")\n",
    "    feature_array = np.array(resized_ela).astype(np.float32) / 255.0\n",
    "    return feature_array\n",
    "\n",
    "\n",
    "def extract_shape_features(image):\n",
    "    kernel_size = 5\n",
    "    transform_iteration = 5\n",
    "\n",
    "    # Define the kernel\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "    image = cv2.resize(image, (224, 224))  # Resize to (224, 224)\n",
    "\n",
    "    image_dict = {}\n",
    "    image_dict[\"original_image\"] = image\n",
    "    image_dict[\"eroded_image\"] = cv2.erode(image_dict[\"original_image\"], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"dilated_image\"] = cv2.dilate(image_dict[\"original_image\"], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"opened_image\"] = cv2.dilate(image_dict['eroded_image'], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"closed_image\"] = cv2.erode(image_dict['dilated_image'], kernel, iterations=transform_iteration)\n",
    "\n",
    "    opened_image_resized = cv2.cvtColor(image_dict[\"opened_image\"], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return opened_image_resized  # Shape: (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(datasets.ImageFolder):\n",
    "    def __init__(self, img_path, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.classes = ['0_real', '1_fake']\n",
    "        self.img_paths = self._get_image_paths()\n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        img_paths = []\n",
    "        for root, dirs, files in os.walk(self.img_path):\n",
    "            for name in files:\n",
    "                if fnmatch.fnmatch(name, \"*.jpg\"):\n",
    "                    label = 0 if '0_real' in root else 1\n",
    "                    img_paths.append((os.path.join(root, name), label))\n",
    "        return img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.img_paths[index]\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img)\n",
    "        \n",
    "        # Ensure the image is now a tensor\n",
    "        if not isinstance(img_tensor, torch.Tensor):\n",
    "            msg = f\"Expected image to be a tensor, but got {type(img_tensor)}.\"\n",
    "            raise TypeError(msg)\n",
    "        \n",
    "        # Convert tensor to numpy array for feature extraction\n",
    "        img_np = img_tensor.numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Extract features\n",
    "        texture_features = extract_texture_features(img_np)\n",
    "        color_features = extract_color_features(img)\n",
    "        shape_features = extract_shape_features(img_np)\n",
    "\n",
    "        img.close()\n",
    "\n",
    "        features = np.stack([texture_features, color_features, shape_features], axis=0)\n",
    "        features = torch.tensor(features).float()\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_dir, batch_size, image_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "\n",
    "    test_dataset = ImageFolderDataset(test_dir, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedSEResNeXt(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ModifiedSEResNeXt, self).__init__()\n",
    "        original_model = timm.create_model('seresnext101_32x4d', pretrained=True)\n",
    "\n",
    "        original_conv1 = original_model.conv1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,  # 3 channels for texture, color, and shape features\n",
    "            out_channels=original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.conv1.weight = nn.Parameter(original_conv1.weight.mean(dim=1, keepdim=True).repeat(1, 3, 1, 1))\n",
    "        \n",
    "        self.bn1 = original_model.bn1\n",
    "        self.act1 = original_model.act1\n",
    "        self.maxpool = original_model.maxpool\n",
    "        self.layer1 = original_model.layer1\n",
    "        self.layer2 = original_model.layer2\n",
    "        self.layer3 = original_model.layer3\n",
    "        self.layer4 = original_model.layer4\n",
    "        self.avg_pool = original_model.global_pool\n",
    "        \n",
    "        num_features = original_model.fc.in_features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_model(filename='seresnext_finetuned.pth', force_new=False):\n",
    "    file_path = os.path.join(os.getcwd(), filename)\n",
    "\n",
    "    # Use pre-existing weights\n",
    "    if os.path.exists(file_path) and not force_new:\n",
    "        model = ModifiedSEResNeXt()\n",
    "        model.load_state_dict(torch.load(file_path, map_location=DEVICE, weights_only=True))\n",
    "        print(f\"Loaded model weights from {file_path}\")\n",
    "        return model\n",
    "    \n",
    "    else:   # Create a new model\n",
    "        # model = create_custom_model(pretrained=True)\n",
    "        model = ModifiedSEResNeXt()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for img, label in test_loader:\n",
    "        # Please make sure that the \"pred\" is binary result\n",
    "        output = model(img.to(DEVICE))\n",
    "        pred = np.argmax(output.detach().to('cpu'), axis=1).numpy()\n",
    "        \n",
    "        y_true.extend(label.numpy())\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    batch = 1\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels).item()\n",
    "            val_loss += loss\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct_no = (predicted == labels).sum().item()\n",
    "            correct += correct_no\n",
    "            batch += 1\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS4487\\.venv\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from c:\\CS4487\\nn\\..\\some_result\\seresnent_e1.pth\n",
      "Accuracy: 0.8960336538461539\n",
      "Loss: 0.2596701517707119, Accuracy: 0.8946\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    model_weight_filename = r'..\\some_result\\seresnent_e1.pth'\n",
    "    \n",
    "    model = get_model(model_weight_filename, force_new=False)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Load the data\n",
    "    test_dir = r'..\\AIGC-Detection-Dataset\\AIGC-Detection-Dataset\\val'\n",
    "    batch_size = 16\n",
    "    image_size = 224\n",
    "    test_loader= load_data(test_dir, batch_size, image_size)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss, acc = validate_model(model, test_loader, criterion)\n",
    "    print(f\"Loss: {loss}, Accuracy: {acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
