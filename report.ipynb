{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting AI-generated content (AIGC)\n",
    "\n",
    "**Author:** LAM Yan Yi, Elaine (57150480), HUNG Kai Hin (57137090), WONG Hoi Fai (57151396)\n",
    "\n",
    "**Date:** 6-Dec-2024\n",
    "\n",
    "## Abstract\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Feature Extraction](#feature-extraction) <br>\n",
    "    1.1 [Error Level Analysis](#error-level-analysis-ela) <br>\n",
    "    1.2 [Morphological Filter](#morphological-filter-mf) <br>\n",
    "    1.3 [Local Binary Pattern](#local-binary-pattern) <br>\n",
    "2. [Data Loader](#data-loader)\n",
    "3. [Model](#model)\n",
    "4. \n",
    "5. [Result](#result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcus/Desktop/Proj/CS4487/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import metrics\n",
    "\n",
    "import zipfile\n",
    "import fnmatch\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "\n",
    "#For texture extraction\n",
    "from skimage import feature\n",
    "import os\n",
    "import csv\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Feature extraction is a process to extract relevant information from raw data. Instead of putting every possible data into the model, a few selected features are used as input for the model. Unrelevant data is disregarded to reduce the drain on computational power and reduce the chances of confusing the network. The simplified data representation makes the training more efficient and quicker to complete, reducing training time.\n",
    "\n",
    "However, choosing the suitable features to extract is no easy feat. Extracting unrelated features puts an unnecessary drain on the system and induces the possibility of confusing the training network. However, not extracting crucial features leaves the network pondering for more useful data and, therefore, unable to map useful connections between the inputs and the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Binary Pattern\n",
    "Local binary pattern is a popular texture feature extraction technique used in the realm of machine learning and data analytics. It is able to provide a strong description of the local texture patterns within an image through comparing the central pixel with its neighbouring pixels to represent in a binary pattern. If the neighbouring pixels cross a threshold and are more intense than the centre pixel, a binary \"1\" is assigned. Through iterative computation of the local binary pattern for each of the pixels on the screen, the resulting output can accurately represent the local textural information stored within the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texture_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    gray_image = gray_image.astype(np.uint8)  # Convert to integer type\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = feature.local_binary_pattern(gray_image, n_points, radius, method='uniform')\n",
    "\n",
    "    lbp = lbp / lbp.max()  # Normalize the LBP values to the range [0, 1]\n",
    "    \n",
    "    return lbp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Level Analysis (ELA)\n",
    "Error level analysis is a common digital forensic technique to recognise images that have been tampered with or digitally altered. Error level analysis detects irregular distributions of quantisation noise to help identify possible regions of the image with a high concentration of inconsistencies in error level across the image.\n",
    "\n",
    "Leveraging the ability of the error level analysis technique, we implemented it into our project as one of our feature extraction methods. It provided our system a way to highlight regions of interest that displayed a significant difference in in error levels. Due to the fact that AI-generated images often do not include natural imperfections found in real photos, it would serve a great purpose in providing information for the network to determine whether it is AI-generated or photorealistic images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_features(image, quality=95, enhance_factor=10):\n",
    "    temp_path = \"temp_recompressed.jpg\"\n",
    "    image.save(temp_path, format=\"JPEG\", quality=quality)\n",
    "    with Image.open(temp_path) as recompressed:\n",
    "        ela_image = ImageChops.difference(image, recompressed)\n",
    "    os.remove(temp_path)\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(ela_image)\n",
    "    enhanced_ela = enhancer.enhance(enhance_factor)\n",
    "\n",
    "    resized_ela = enhanced_ela.resize((224, 224)).convert(\"L\")\n",
    "    feature_array = np.array(resized_ela).astype(np.float32) / 255.0\n",
    "    return feature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Filter (MF)\n",
    "Morphological filters are a class of image processing techniques used to analyse and process the shapes and structures within an image. There are two types of basic morphological filters as well as two advanced morphological filters that utilise the two basic ones in conjunction.\n",
    "\n",
    "The first basic morphological filter is erosion. A morphological erosion filter is a robust technique for shrinking the boundaries of objects within an image. It decreases the amount of bright regions while increasing the amount of dark regions. By decreasing the overall amount of bright regions, it is able to erode away the brighter regions at the boundaries of the objects, effectively pushing the boundaries inward towards the centre of the object space. This process is done iteratively, resulting in an image with a greatly reduced amount of bright regions. This reduction in bright pixels is achieved through the iterative application of a minimising kernel. After applying a mask or a kernel to the image, the output pixel is calculated to be the minimum value of all the values within the masked area. The resulting output image has a strong tendency to separate overlapping objects as each boundary of the objects is shrunk inwards respectively.\n",
    "\n",
    "The other form of basic morphological filter is dilation. A morphological dilation filter is highly capable in expanding the region of interest within an image. Reflecting a strong contrast from that of a morphological erosion filter, it increases the amount of bright regions while decreasing the amount of dark regions. By increasing the overall amount of bright areas, it is able to fill in small gaps and holes within objects in the image. On a large enough scale, it can even connect disjointed parts of the same object by enlarging and thickening the object's visual boundaries. The boundaries are seen as moving outwards, away from the centre of the object. This process is done iteratively, resulting in an image with a greatly reduced amount of dark regions. The increase in bright pixels is achieved through the iterative application of a maximising kernel. After the maximising kernel is applied, the output pixel is calculated to be the maximum value of all the values contained in the kernel area. The resulting output image will have a high likelihood of filling small gaps in the object. The added benefit of a morphological dilation filter is its ability to smooth out rough boundaries.\n",
    "\n",
    "By combining the two basic types of morphological filters, two advanced types of morphological filters came to fruition: Opening and Closing.\n",
    "\n",
    "Opening combines erosion and dilation, where the input image first passes through a morphological erosion filter and then passes through a morphological dilation filter. Through the specific ordered combination of erosion and dilation, it is able to break apart narrow gaps between objects. An added benefit is small objects will be covered by the filter, allowing extraction only of the important major objects.\n",
    "\n",
    "Closing also combines both erosion and dilation. However, it encompasses both in a different order. Unlike opening, it first uses a morphological dilation filter, and then the resulting image is passed through a morphological erosion filter. Through the specific ordered combination of erosion and dilation, it is able to close small breaks.\n",
    "\n",
    "Through careful consideration, we have decided to use opening as our method of feature extraction for our detection between AI-generated content and photorealistic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shape_features(image):\n",
    "    kernel_size = 5\n",
    "    transform_iteration = 5\n",
    "\n",
    "    # Define the kernel\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "    image = cv2.resize(image, (224, 224))  # Resize to (224, 224)\n",
    "\n",
    "    image_dict = {}\n",
    "    image_dict[\"original_image\"] = image\n",
    "    image_dict[\"eroded_image\"] = cv2.erode(image_dict[\"original_image\"], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"dilated_image\"] = cv2.dilate(image_dict[\"original_image\"], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"opened_image\"] = cv2.dilate(image_dict['eroded_image'], kernel, iterations=transform_iteration)\n",
    "    image_dict[\"closed_image\"] = cv2.erode(image_dict['dilated_image'], kernel, iterations=transform_iteration)\n",
    "\n",
    "    opened_image_resized = cv2.cvtColor(image_dict[\"opened_image\"], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return opened_image_resized  # Shape: (224, 224)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_zip(zip_path, img_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        with zf.open(img_path) as file:\n",
    "            img = Image.open(file)\n",
    "            return img.convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "        \n",
    "class ZipImageFolderDataset(datasets.ImageFolder):\n",
    "    def __init__(self, zip_path, root, transform=None):\n",
    "        self.zip_path = zip_path\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.classes = ['0_real', '1_fake']\n",
    "        self.img_paths = self._get_image_paths()\n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        img_paths = []\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zf:\n",
    "            for file_info in zf.infolist():\n",
    "                name = file_info.filename\n",
    "                if fnmatch.fnmatch(name, f\"{self.root}/*.jpg\"):\n",
    "                    label = 0 if '0_real' in name.split('/')[1] else 1\n",
    "                    img_paths.append((name, label))\n",
    "        return img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.img_paths[index]\n",
    "        img = load_image_from_zip(self.zip_path, img_path)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img)\n",
    "        \n",
    "        # Ensure the image is now a tensor\n",
    "        if not isinstance(img_tensor, torch.Tensor):\n",
    "            raise TypeError(f\"Expected image to be a tensor, but got {type(img_tensor)}.\")\n",
    "        \n",
    "        # Convert tensor to numpy array for feature extraction\n",
    "        img_np = img_tensor.numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Extract features\n",
    "        texture_features = extract_texture_features(img_np)\n",
    "        color_features = extract_color_features(img)\n",
    "        shape_features = extract_shape_features(img_np)\n",
    "        \n",
    "        features = np.stack([texture_features, color_features, shape_features], axis=0)\n",
    "        features = torch.tensor(features).float().permute(1, 2, 0)  # Change the shape to [height, width, channels]\n",
    "        features = features.permute(2, 0, 1)  # Change the shape to [channels, height, width]\n",
    "        \n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(datasets.ImageFolder):\n",
    "    def __init__(self, zip_path, root, transform=None):\n",
    "        self.zip_path = zip_path\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.classes = ['0_real', '1_fake']\n",
    "        self.img_paths = self._get_image_paths()\n",
    "\n",
    "    def _get_image_paths(roots):\n",
    "        img_paths = []\n",
    "        for root, dirs, files in os.walk(roots):\n",
    "            for name in files:\n",
    "                if fnmatch.fnmatch(name, \"*.jpg\"):\n",
    "                    label = 0 if '0_real' in root else 1\n",
    "                    img_paths.append((os.path.join(root, name), label))\n",
    "        return img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.img_paths[index]\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img)\n",
    "        \n",
    "        # Ensure the image is now a tensor\n",
    "        if not isinstance(img_tensor, torch.Tensor):\n",
    "            raise TypeError(f\"Expected image to be a tensor, but got {type(img_tensor)}.\")\n",
    "        \n",
    "        # Convert tensor to numpy array for feature extraction\n",
    "        img_np = img_tensor.numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Extract features\n",
    "        texture_features = extract_texture_features(img_np)\n",
    "        color_features = extract_color_features(img)\n",
    "        shape_features = extract_shape_features(img_np)\n",
    "        \n",
    "        features = np.stack([texture_features, color_features, shape_features], axis=0)\n",
    "        features = torch.tensor(features).float().permute(1, 2, 0)  # Change the shape to [height, width, channels]\n",
    "        features = features.permute(2, 0, 1)  # Change the shape to [channels, height, width]\n",
    "        \n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(roots):\n",
    "    img_paths = []\n",
    "    for root, dirs, files in os.walk(roots):\n",
    "        for name in files:\n",
    "            if fnmatch.fnmatch(name, \"*.jpg\"):\n",
    "                label = 0 if '0_real' in root else 1\n",
    "                img_paths.append((os.path.join(root, name), label))\n",
    "    return img_paths\n",
    "\n",
    "imgs = get_image_paths(r'../AIGC-Detection-Dataset')\n",
    "# print(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(zip_path, batch_size, image_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_dir = \"AIGC-Detection-Dataset/train\"\n",
    "    val_dir = \"AIGC-Detection-Dataset/val\"\n",
    "    # test_dir = \"AIGC-Detection-Dataset/val\"\n",
    "\n",
    "    train_dataset = ZipImageFolderDataset(zip_path, train_dir, transform=transform)\n",
    "    val_dataset = ZipImageFolderDataset(zip_path, val_dir, transform=transform)\n",
    "    # test_dataset = ZipImageFolderDataset(zip_path, test_dir, transform=transform)\n",
    "    print(f\"Data prepared:\\nTrain: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "    print(\"Data loaded\")\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    batch = 0\n",
    "    print(f\"Total batches: {len(test_loader)}\")\n",
    "    for img, label in test_loader:\n",
    "        # Please make sure that the \"pred\" is binary result\n",
    "        output = model(img.to(DEVICE))\n",
    "        pred = np.argmax(output.detach().to('cpu'), axis=1).numpy()\n",
    "        \n",
    "        y_true.extend(label.numpy())\n",
    "        y_pred.extend(pred)\n",
    "        \n",
    "        print(f\"Batch: {batch} completed\")\n",
    "        batch += 1\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    print(f'Validation Accuracy: {accuracy}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_without_improvement = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        print(f\"Epoch {epoch+1} started...\")\n",
    "        print(f\"length of train_loader: {len(train_loader)}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        batch = 1\n",
    "        for features, labels in train_loader:\n",
    "            # images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            features = features.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            print(f'Batch {batch} completed...')\n",
    "            batch += 1\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Time: {time.time()-start_time:.2f}s')\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            # Save the best model weights (optional)\n",
    "            save_model(model)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss.\")\n",
    "            break\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path='testing.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    # for name, param in model.state_dict().items():\n",
    "    #     with open(r\"weights.txt\",'a') as file:\n",
    "    #         file.write(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")  \n",
    "    # with open(r\"weights.txt\",'a') as file:\n",
    "    #     file.write('-'*100 + '\\n') \n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(train_losses, val_losses, val_accuracies, path='results.csv'):\n",
    "    with open(path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train Loss', 'Val Loss', 'Val Accuracy'])\n",
    "        for i in range(len(train_losses)):\n",
    "            writer.writerow([i+1, train_losses[i], val_losses[i], val_accuracies[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE).float().unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predicted = (outputs > 0.5).int()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_model(pretrained):\n",
    "    model = timm.create_model('seresnext101_32x4d', pretrained)\n",
    "    \n",
    "    # Modify the first convolutional layer\n",
    "    original_conv1 = model.conv1\n",
    "    new_conv1 = nn.Conv2d(3, original_conv1.out_channels, kernel_size=original_conv1.kernel_size,\n",
    "                        stride=original_conv1.stride, padding=original_conv1.padding, bias=False)\n",
    "    with torch.no_grad():\n",
    "        new_conv1.weight[:, :3, :, :] = original_conv1.weight[:, :3, :, :]\n",
    "    model.conv1 = new_conv1\n",
    "\n",
    "    # Adjust the final layer for binary classification\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(filename='seresnext_finetuned.pth', force_new=False):\n",
    "    file_path = os.path.join(os.getcwd(), filename)\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Use pre-existing weights\n",
    "    if os.path.exists(file_path) and not force_new:\n",
    "        model = create_custom_model(pretrained=False)\n",
    "        model.load_state_dict(torch.load(file_path, map_location=DEVICE, weights_only=True))\n",
    "        print(\"Model loaded successfully!\")\n",
    "        return model\n",
    "    \n",
    "    else:   # Create a new model\n",
    "        model = create_custom_model(pretrained=True)\n",
    "        print(\"Creating a new model\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new model\n",
      "Data prepared:\n",
      "Train: 45000, Val: 5000\n",
      "Data loaded\n",
      "Epoch 1 started...\n",
      "length of train_loader: 703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m train_losses, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the results to a CSV file\u001b[39;00m\n\u001b[1;32m     19\u001b[0m save_results(train_losses, val_losses, val_accuracies)\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# images = images.to(DEVICE)\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Proj/CS4487/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/Proj/CS4487/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/Proj/CS4487/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mZipImageFolderDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     29\u001b[0m     img_path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_paths[index]\n\u001b[0;32m---> 30\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_image_from_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     32\u001b[0m         img_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mload_image_from_zip\u001b[0;34m(zip_path, img_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image_from_zip\u001b[39m(zip_path, img_path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zf:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(img_path) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m             img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(file)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py:1349\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py:1474\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1471\u001b[0m x\u001b[38;5;241m.\u001b[39m_raw_time \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m   1472\u001b[0m x\u001b[38;5;241m.\u001b[39mdate_time \u001b[38;5;241m=\u001b[39m ( (d\u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m9\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1980\u001b[39m, (d\u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0xF\u001b[39m, d\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0x1F\u001b[39m,\n\u001b[1;32m   1473\u001b[0m                 t\u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m11\u001b[39m, (t\u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0x3F\u001b[39m, (t\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0x1F\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m )\n\u001b[0;32m-> 1474\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decodeExtra\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_filename_crc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m x\u001b[38;5;241m.\u001b[39mheader_offset \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mheader_offset \u001b[38;5;241m+\u001b[39m concat\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelist\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py:505\u001b[0m, in \u001b[0;36mZipInfo._decodeExtra\u001b[0;34m(self, filename_crc)\u001b[0m\n\u001b[1;32m    503\u001b[0m unpack \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(extra) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 505\u001b[0m     tp, ln \u001b[38;5;241m=\u001b[39m \u001b[43munpack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<HH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ln\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(extra):\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrupt extra field \u001b[39m\u001b[38;5;132;01m%04x\u001b[39;00m\u001b[38;5;124m (size=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tp, ln))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_weight_filename = 'seresnext_finetuned.pth'\n",
    "\n",
    "model = get_model(model_weight_filename, force_new=False)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Load the data\n",
    "zip_path = './AIGC-Detection-Dataset.zip'\n",
    "batch_size = 64\n",
    "image_size = 224\n",
    "train_loader, val_loader = load_data(zip_path, batch_size, image_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "train_losses, val_losses, val_accuracies = train_model(model, train_loader, val_loader, optimizer, criterion, 5)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "save_results(train_losses, val_losses, val_accuracies)\n",
    "\n",
    "save_model(model, model_weight_filename)\n",
    "# evaluate(model, train_loader)\n",
    "# evaluate(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "The first submission rate:<br><br>\n",
    "Intra-domain Test Accuracy: 0.9224<br>\n",
    "Cross-domain Test Accuracy: 0.8422<br>\n",
    "Average Test Accuracy: 0.8823<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
